{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "large_IMPALA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CM134/RL_Project_02456/blob/large_impala/large_IMPALA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1_WKdcrI6w3"
      },
      "source": [
        "# Getting started with PPO and ProcGen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7LP1JU3I-d4"
      },
      "source": [
        "Here's a bit of code that should help you get started on your projects.\n",
        "\n",
        "The cell below installs `procgen` and downloads a small `utils.py` script that contains some utility functions. You may want to inspect the file for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdpZ4lmFHtD8",
        "outputId": "1695cf39-9a5f-4898-e079-85cf8d3d60f3"
      },
      "source": [
        "!pip uninstall imgaug\n",
        "!pip install imgaug==0.2.6\n",
        "\n",
        "!pip install procgen\n",
        "!wget https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: imgaug 0.2.6\n",
            "Uninstalling imgaug-0.2.6:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/imgaug-0.2.6.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/imgaug/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled imgaug-0.2.6\n",
            "Collecting imgaug==0.2.6\n",
            "  Using cached imgaug-0.2.6-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (0.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (1.15.0)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (8.4.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (2.11.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (2.6.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (1.2.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (3.0.6)\n",
            "Installing collected packages: imgaug\n",
            "Successfully installed imgaug-0.2.6\n",
            "Requirement already satisfied: procgen in /usr/local/lib/python3.7/dist-packages (0.10.4)\n",
            "Requirement already satisfied: gym<1.0.0,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from procgen) (0.17.3)\n",
            "Requirement already satisfied: gym3<1.0.0,>=0.3.3 in /usr/local/lib/python3.7/dist-packages (from procgen) (0.3.3)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from procgen) (1.19.5)\n",
            "Requirement already satisfied: filelock<4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from procgen) (3.4.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.4.1)\n",
            "Requirement already satisfied: imageio-ffmpeg<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (0.3.0)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.15.0)\n",
            "Requirement already satisfied: imageio<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (2.11.1)\n",
            "Requirement already satisfied: moderngl<6.0.0,>=5.5.4 in /usr/local/lib/python3.7/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (5.6.4)\n",
            "Requirement already satisfied: glfw<2.0.0,>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.12.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi<2.0.0,>=1.13.0->gym3<1.0.0,>=0.3.3->procgen) (2.21)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.7/dist-packages (from imageio<3.0.0,>=2.6.0->gym3<1.0.0,>=0.3.3->procgen) (8.4.0)\n",
            "Requirement already satisfied: glcontext<3,>=2 in /usr/local/lib/python3.7/dist-packages (from moderngl<6.0.0,>=5.5.4->gym3<1.0.0,>=0.3.3->procgen) (2.3.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<1.0.0,>=0.15.0->procgen) (0.16.0)\n",
            "--2021-11-22 20:25:35--  https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14807 (14K) [text/plain]\n",
            "Saving to: ‘utils.py.3’\n",
            "\n",
            "utils.py.3          100%[===================>]  14.46K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-11-22 20:25:35 (78.1 MB/s) - ‘utils.py.3’ saved [14807/14807]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn2rkllGJPtZ"
      },
      "source": [
        "Hyperparameters. These values should be a good starting point. You can modify them later once you have a working implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z8P1ehENCwc"
      },
      "source": [
        "# Hyperparameters\n",
        "total_steps = 1000000\n",
        "num_envs = 32\n",
        "num_levels = 10\n",
        "num_steps = 256\n",
        "num_epochs = 3\n",
        "batch_size = 512\n",
        "eps = .2\n",
        "grad_eps = .5\n",
        "value_coef = .5\n",
        "entropy_coef = .01\n",
        "feature_dim = 256\n",
        "\n",
        "\n",
        "# Environment \n",
        "envname = \"coinrun\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxRWy_T9JY4M"
      },
      "source": [
        "Network definitions. We have defined a policy network for you in advance. It uses the popular `NatureDQN` encoder architecture (see below), while policy and value functions are linear projections from the encodings. There is plenty of opportunity to experiment with architectures, so feel free to do that! Perhaps implement the `Impala` encoder from [this paper](https://arxiv.org/pdf/1802.01561.pdf) (perhaps minus the LSTM)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTBV9xpKpEFa",
        "outputId": "e2dc4fd1-72ab-45c2-dd91-92b41b37e621"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from utils import make_env, Storage, orthogonal_init\n",
        "\n",
        "\n",
        "def xavier_uniform_init(module, gain=1.0):\n",
        "    if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
        "        nn.init.xavier_uniform_(module.weight.data, gain)\n",
        "        nn.init.constant_(module.bias.data, 0)\n",
        "    return module\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, in_channels, feature_dim):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4), nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1), nn.ReLU(),\n",
        "        Flatten(),\n",
        "        nn.Linear(in_features=1024, out_features=feature_dim), nn.ReLU()\n",
        "    )\n",
        "    self.apply(orthogonal_init)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "\n",
        "# Large IMPALA encoder \n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, in_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv2 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = nn.ReLU()(x)\n",
        "    out = self.conv1(out)\n",
        "    out = nn.ReLU()(out)\n",
        "    out = self.conv2(out)\n",
        "    return out + x\n",
        "\n",
        "\n",
        "class ImpalaBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
        "    self.res1 = ResidualBlock(out_channels)\n",
        "    self.res2 = ResidualBlock(out_channels)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)(x)\n",
        "    x = self.res1(x)\n",
        "    x = self.res2(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class ImpalaModel(nn.Module):\n",
        "  def __init__(self, in_channels, feature_dim):\n",
        "    super().__init__()\n",
        "    self.imp1 = ImpalaBlock(in_channels=in_channels, out_channels=16)\n",
        "    self.imp2 = ImpalaBlock(in_channels=16, out_channels=32)\n",
        "    self.imp3 = ImpalaBlock(in_channels=32, out_channels=32)\n",
        "    self.fc1 = nn.Linear(in_features=32*8*8, out_features=feature_dim)\n",
        "    self.output_dim = 256\n",
        "    self.apply(xavier_uniform_init)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.imp1(x)\n",
        "    x = self.imp2(x)\n",
        "    x = self.imp3(x)\n",
        "    x = nn.ReLU()(x)\n",
        "    x = Flatten()(x)\n",
        "    x = self.fc1(x)\n",
        "    x = nn.ReLU()(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Policy(nn.Module):\n",
        "  def __init__(self, encoder, feature_dim, num_actions):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)\n",
        "    self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)\n",
        "\n",
        "  def act(self, x):\n",
        "    with torch.no_grad():\n",
        "      x = x.cuda().contiguous()\n",
        "      dist, value = self.forward(x)\n",
        "      action = dist.sample()\n",
        "      log_prob = dist.log_prob(action)\n",
        "    \n",
        "    return action.cpu(), log_prob.cpu(), value.cpu()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    logits = self.policy(x)\n",
        "    value = self.value(x).squeeze(1)\n",
        "    dist = torch.distributions.Categorical(logits=logits)\n",
        "\n",
        "    return dist, value\n",
        "\n",
        "\n",
        "# Define environment\n",
        "# check the utils.py file for info on arguments\n",
        "env = make_env(num_envs, num_levels=num_levels, env_name=envname)\n",
        "\n",
        "print('Observation space:', env.observation_space)\n",
        "print('Action space:', env.action_space.n)\n",
        "\n",
        "\n",
        "# Define network\n",
        "#encoder = Encoder(in_channels=env.observation_space.shape[0], feature_dim=feature_dim)\n",
        "encoder = ImpalaModel(in_channels=env.observation_space.shape[0], feature_dim=feature_dim)\n",
        "policy = Policy(encoder=encoder, feature_dim=feature_dim, num_actions=env.action_space.n)\n",
        "policy.cuda()\n",
        "\n",
        "# Define optimizer\n",
        "# these are reasonable values but probably not optimal\n",
        "optimizer = torch.optim.Adam(policy.parameters(), lr=5e-4, eps=1e-5)\n",
        "\n",
        "# Define temporary storage\n",
        "# we use this to collect transitions during each iteration\n",
        "storage = Storage(\n",
        "    env.observation_space.shape,\n",
        "    num_steps,\n",
        "    num_envs\n",
        ")\n",
        "\n",
        "# Run training\n",
        "obs = env.reset()\n",
        "step = 0\n",
        "while step < total_steps:\n",
        "\n",
        "  # Use policy to collect data for num_steps steps\n",
        "  policy.eval()\n",
        "  for _ in range(num_steps):\n",
        "    # Use policy\n",
        "    action, log_prob, value = policy.act(obs)\n",
        "    \n",
        "    # Take step in environment\n",
        "    next_obs, reward, done, info = env.step(action)\n",
        "\n",
        "    # Store data\n",
        "    storage.store(obs, action, reward, done, info, log_prob, value)\n",
        "    \n",
        "    # Update current observation\n",
        "    obs = next_obs\n",
        "\n",
        "  # Add the last observation to collected data\n",
        "  _, _, value = policy.act(obs)\n",
        "  storage.store_last(obs, value)\n",
        "\n",
        "  # Compute return and advantage\n",
        "  storage.compute_return_advantage()\n",
        "\n",
        "  # Optimize policy\n",
        "  policy.train()\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    # Iterate over batches of transitions\n",
        "    generator = storage.get_generator(batch_size)\n",
        "    for batch in generator:\n",
        "      b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch\n",
        "\n",
        "      # Get current policy outputs\n",
        "      new_dist, new_value = policy(b_obs)\n",
        "      new_log_prob = new_dist.log_prob(b_action)\n",
        "\n",
        "      # Clipped policy objective\n",
        "      ratio = torch.exp((new_log_prob - b_log_prob))\n",
        "      surr1 = ratio * b_advantage\n",
        "      surr2 = torch.clamp(ratio, 1.0 - eps, 1.0 + eps) * b_advantage\n",
        "      pi_loss = - torch.min(surr1, surr2)\n",
        "\n",
        "      # Clipped value function objective\n",
        "      value_loss = value_coef * (b_returns - new_value).pow(2)\n",
        "\n",
        "      # Entropy loss\n",
        "      entropy_loss = entropy_coef * new_dist.entropy()\n",
        "\n",
        "      # Backpropagate losses\n",
        "      loss = pi_loss + value_loss - entropy_loss\n",
        "      loss.mean().backward()\n",
        "\n",
        "      # Clip gradients\n",
        "      torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)\n",
        "\n",
        "      # Update policy\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "  # Update stats\n",
        "  step += num_envs * num_steps\n",
        "  print(f'Step: {step}\\tMean reward: {storage.get_reward()}')\n",
        "\n",
        "print('Completed training!')\n",
        "torch.save(policy.state_dict, 'checkpoint.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space: Box(0.0, 1.0, (3, 64, 64), float32)\n",
            "Action space: 15\n",
            "Step: 8192\tMean reward: 0.0\n",
            "Step: 16384\tMean reward: 0.3125\n",
            "Step: 24576\tMean reward: 0.625\n",
            "Step: 32768\tMean reward: 1.5625\n",
            "Step: 40960\tMean reward: 4.6875\n",
            "Step: 49152\tMean reward: 7.1875\n",
            "Step: 57344\tMean reward: 7.5\n",
            "Step: 65536\tMean reward: 9.6875\n",
            "Step: 73728\tMean reward: 9.0625\n",
            "Step: 81920\tMean reward: 11.25\n",
            "Step: 90112\tMean reward: 12.5\n",
            "Step: 98304\tMean reward: 10.625\n",
            "Step: 106496\tMean reward: 14.0625\n",
            "Step: 114688\tMean reward: 15.9375\n",
            "Step: 122880\tMean reward: 15.625\n",
            "Step: 131072\tMean reward: 19.0625\n",
            "Step: 139264\tMean reward: 20.625\n",
            "Step: 147456\tMean reward: 20.0\n",
            "Step: 155648\tMean reward: 23.4375\n",
            "Step: 163840\tMean reward: 24.6875\n",
            "Step: 172032\tMean reward: 24.0625\n",
            "Step: 180224\tMean reward: 27.8125\n",
            "Step: 188416\tMean reward: 18.4375\n",
            "Step: 196608\tMean reward: 25.0\n",
            "Step: 204800\tMean reward: 22.5\n",
            "Step: 212992\tMean reward: 29.375\n",
            "Step: 221184\tMean reward: 25.625\n",
            "Step: 229376\tMean reward: 21.5625\n",
            "Step: 237568\tMean reward: 24.375\n",
            "Step: 245760\tMean reward: 26.25\n",
            "Step: 253952\tMean reward: 28.125\n",
            "Step: 262144\tMean reward: 25.625\n",
            "Step: 270336\tMean reward: 33.4375\n",
            "Step: 278528\tMean reward: 28.125\n",
            "Step: 286720\tMean reward: 27.8125\n",
            "Step: 294912\tMean reward: 24.6875\n",
            "Step: 303104\tMean reward: 30.625\n",
            "Step: 311296\tMean reward: 29.375\n",
            "Step: 319488\tMean reward: 32.5\n",
            "Step: 327680\tMean reward: 26.875\n",
            "Step: 335872\tMean reward: 27.1875\n",
            "Step: 344064\tMean reward: 27.1875\n",
            "Step: 352256\tMean reward: 30.625\n",
            "Step: 360448\tMean reward: 31.875\n",
            "Step: 368640\tMean reward: 32.8125\n",
            "Step: 376832\tMean reward: 33.75\n",
            "Step: 385024\tMean reward: 30.3125\n",
            "Step: 393216\tMean reward: 32.1875\n",
            "Step: 401408\tMean reward: 31.5625\n",
            "Step: 409600\tMean reward: 30.625\n",
            "Step: 417792\tMean reward: 33.75\n",
            "Step: 425984\tMean reward: 32.8125\n",
            "Step: 434176\tMean reward: 35.0\n",
            "Step: 442368\tMean reward: 34.375\n",
            "Step: 450560\tMean reward: 35.0\n",
            "Step: 458752\tMean reward: 35.0\n",
            "Step: 466944\tMean reward: 31.5625\n",
            "Step: 475136\tMean reward: 35.3125\n",
            "Step: 483328\tMean reward: 35.9375\n",
            "Step: 491520\tMean reward: 36.25\n",
            "Step: 499712\tMean reward: 37.1875\n",
            "Step: 507904\tMean reward: 37.8125\n",
            "Step: 516096\tMean reward: 37.1875\n",
            "Step: 524288\tMean reward: 36.25\n",
            "Step: 532480\tMean reward: 34.375\n",
            "Step: 540672\tMean reward: 34.6875\n",
            "Step: 548864\tMean reward: 40.3125\n",
            "Step: 557056\tMean reward: 37.5\n",
            "Step: 565248\tMean reward: 42.5\n",
            "Step: 573440\tMean reward: 38.4375\n",
            "Step: 581632\tMean reward: 41.25\n",
            "Step: 589824\tMean reward: 38.75\n",
            "Step: 598016\tMean reward: 39.0625\n",
            "Step: 606208\tMean reward: 37.1875\n",
            "Step: 614400\tMean reward: 39.0625\n",
            "Step: 622592\tMean reward: 34.0625\n",
            "Step: 630784\tMean reward: 39.0625\n",
            "Step: 638976\tMean reward: 37.5\n",
            "Step: 647168\tMean reward: 39.0625\n",
            "Step: 655360\tMean reward: 37.5\n",
            "Step: 663552\tMean reward: 39.0625\n",
            "Step: 671744\tMean reward: 38.125\n",
            "Step: 679936\tMean reward: 40.9375\n",
            "Step: 688128\tMean reward: 44.375\n",
            "Step: 696320\tMean reward: 41.875\n",
            "Step: 704512\tMean reward: 41.25\n",
            "Step: 712704\tMean reward: 40.9375\n",
            "Step: 720896\tMean reward: 39.6875\n",
            "Step: 729088\tMean reward: 40.625\n",
            "Step: 737280\tMean reward: 39.375\n",
            "Step: 745472\tMean reward: 37.8125\n",
            "Step: 753664\tMean reward: 39.0625\n",
            "Step: 761856\tMean reward: 40.625\n",
            "Step: 770048\tMean reward: 40.625\n",
            "Step: 778240\tMean reward: 42.1875\n",
            "Step: 786432\tMean reward: 41.25\n",
            "Step: 794624\tMean reward: 42.8125\n",
            "Step: 802816\tMean reward: 40.625\n",
            "Step: 811008\tMean reward: 40.9375\n",
            "Step: 819200\tMean reward: 41.25\n",
            "Step: 827392\tMean reward: 38.4375\n",
            "Step: 835584\tMean reward: 44.375\n",
            "Step: 843776\tMean reward: 40.0\n",
            "Step: 851968\tMean reward: 43.125\n",
            "Step: 860160\tMean reward: 44.6875\n",
            "Step: 868352\tMean reward: 43.4375\n",
            "Step: 876544\tMean reward: 37.5\n",
            "Step: 884736\tMean reward: 41.25\n",
            "Step: 892928\tMean reward: 40.0\n",
            "Step: 901120\tMean reward: 38.4375\n",
            "Step: 909312\tMean reward: 43.75\n",
            "Step: 917504\tMean reward: 41.5625\n",
            "Step: 925696\tMean reward: 40.625\n",
            "Step: 933888\tMean reward: 41.25\n",
            "Step: 942080\tMean reward: 42.1875\n",
            "Step: 950272\tMean reward: 41.875\n",
            "Step: 958464\tMean reward: 44.6875\n",
            "Step: 966656\tMean reward: 40.9375\n",
            "Step: 974848\tMean reward: 42.1875\n",
            "Step: 983040\tMean reward: 43.75\n",
            "Step: 991232\tMean reward: 44.6875\n",
            "Step: 999424\tMean reward: 41.875\n",
            "Step: 1007616\tMean reward: 45.0\n",
            "Completed training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAZrWuVGLTu-"
      },
      "source": [
        "Below cell can be used for policy evaluation and saves an episode to mp4 for you to view."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zecOCkd7Jzt",
        "outputId": "c4e3390e-82a2-4c8c-eaf6-6809db415c3a"
      },
      "source": [
        "import imageio\n",
        "\n",
        "# Make evaluation environment\n",
        "eval_env = make_env(num_envs, start_level=num_levels, num_levels=num_levels, env_name=envname)\n",
        "obs = eval_env.reset()\n",
        "\n",
        "frames = []\n",
        "total_reward = []\n",
        "\n",
        "# Evaluate policy\n",
        "policy.eval()\n",
        "for _ in range(512):\n",
        "\n",
        "  # Use policy\n",
        "  action, log_prob, value = policy.act(obs)\n",
        "\n",
        "  # Take step in environment\n",
        "  obs, reward, done, info = eval_env.step(action)\n",
        "  total_reward.append(torch.Tensor(reward))\n",
        "\n",
        "  # Render environment and store\n",
        "  frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()\n",
        "  frames.append(frame)\n",
        "\n",
        "# Calculate average return\n",
        "total_reward = torch.stack(total_reward).sum(0).mean(0)\n",
        "print('Average return:', total_reward)\n",
        "\n",
        "# Save frames as video\n",
        "frames = torch.stack(frames)\n",
        "imageio.mimsave('vid.mp4', frames, fps=25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average return: tensor(56.7062)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SkKBuYrgnkn"
      },
      "source": [
        "#Average return: tensor(13.2928)\n",
        "\n",
        "#Step: 8192\tMean reward: 3.75\n",
        "#Step: 16384\tMean reward: 4.21875\n",
        "#Step: 24576\tMean reward: 3.5\n",
        "#Step: 32768\tMean reward: 4.28125\n",
        "#Step: 40960\tMean reward: 4.375\n",
        "#Step: 49152\tMean reward: 5.15625\n",
        "#Step: 57344\tMean reward: 5.28125"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU-d2A10gd_z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}