{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1_WKdcrI6w3"
   },
   "source": [
    "# Getting started with PPO and ProcGen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7LP1JU3I-d4"
   },
   "source": [
    "Here's a bit of code that should help you get started on your projects.\n",
    "\n",
    "The cell below installs `procgen` and downloads a small `utils.py` script that contains some utility functions. You may want to inspect the file for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KdpZ4lmFHtD8",
    "outputId": "530a4764-9fb7-4c7a-ca8b-ec3dc42951b2"
   },
   "outputs": [],
   "source": [
    "#!pip install procgen\n",
    "#!wget https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bn2rkllGJPtZ"
   },
   "source": [
    "Hyperparameters. These values should be a good starting point. You can modify them later once you have a working implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "8Z8P1ehENCwc"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "total_steps = 4e6\n",
    "num_envs = 32\n",
    "num_levels = 10\n",
    "num_steps = 256\n",
    "num_epochs = 3\n",
    "batch_size = 512\n",
    "eps = .2\n",
    "grad_eps = .5\n",
    "value_coef = .5\n",
    "entropy_coef = .01\n",
    "\n",
    "feature_dim = 256\n",
    "envname = 'starpilot'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxRWy_T9JY4M"
   },
   "source": [
    "Network definitions. We have defined a policy network for you in advance. It uses the popular `NatureDQN` encoder architecture (see below), while policy and value functions are linear projections from the encodings. There is plenty of opportunity to experiment with architectures, so feel free to do that! Perhaps implement the `Impala` encoder from [this paper](https://arxiv.org/pdf/1802.01561.pdf) (perhaps minus the LSTM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "yTBV9xpKpEFa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import make_env, Storage, orthogonal_init\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, in_channels, feature_dim):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=8, stride=4), nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2), nn.ReLU(),\n",
    "        Flatten(),\n",
    "        nn.Linear(in_features=1152, out_features=256), nn.ReLU()\n",
    "    )\n",
    "    self.LSTM = nn.Sequential(\n",
    "        nn.LSTM(input_size=256, hidden_size=256, num_layers=1)\n",
    "    )\n",
    "    self.apply(orthogonal_init)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out=self.layers(x)\n",
    "    out = out.view(1,out.shape[0],out.shape[1])\n",
    "    out,(h_n, c_n) = self.LSTM(out)\n",
    "    #print(out.shape)\n",
    "    \n",
    "    return out.squeeze(0)\n",
    "\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "  def __init__(self, encoder, feature_dim, num_actions):\n",
    "    super().__init__()\n",
    "    self.encoder = encoder\n",
    "    self.policy = nn.Sequential(\n",
    "        nn.Linear(feature_dim, 256), nn.ReLU(),\n",
    "        nn.Linear(256, 126), nn.ReLU(),\n",
    "        nn.Linear(126, num_actions)\n",
    "    )\n",
    "    self.value = nn.Sequential(\n",
    "        nn.Linear(feature_dim, 256), nn.ReLU(),\n",
    "        nn.Linear(feature_dim, 1)\n",
    "    )\n",
    "\n",
    "  def act(self, x):\n",
    "    with torch.no_grad():\n",
    "      x = x.cuda().contiguous()\n",
    "      x = x.contiguous()\n",
    "      dist, value = self.forward(x)\n",
    "      action = dist.sample()\n",
    "      log_prob = dist.log_prob(action)\n",
    "    \n",
    "    return action.cpu(), log_prob.cpu(), value.cpu()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder(x)\n",
    "    logits = self.policy(x)\n",
    "    value = self.value(x).squeeze(1)\n",
    "    dist = torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "    return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-GCXwFfyRxs",
    "outputId": "c42b5182-4ba6-4b5a-f68e-91f2ff4986c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box([[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]], [[[1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]]], (3, 64, 64), float32)\n",
      "Action space: 15\n",
      "Step: 8192\tMean reward: 3.75\n",
      "Step: 16384\tMean reward: 4.40625\n",
      "Step: 24576\tMean reward: 3.40625\n",
      "Step: 32768\tMean reward: 4.59375\n",
      "Step: 40960\tMean reward: 3.84375\n",
      "Step: 49152\tMean reward: 4.03125\n",
      "Step: 57344\tMean reward: 4.8125\n",
      "Step: 65536\tMean reward: 5.125\n",
      "Step: 73728\tMean reward: 4.15625\n",
      "Step: 81920\tMean reward: 5.78125\n",
      "Step: 90112\tMean reward: 5.125\n",
      "Step: 98304\tMean reward: 5.9375\n",
      "Step: 106496\tMean reward: 6.375\n",
      "Step: 114688\tMean reward: 6.1875\n",
      "Step: 122880\tMean reward: 7.875\n",
      "Step: 131072\tMean reward: 9.46875\n",
      "Step: 139264\tMean reward: 7.25\n",
      "Step: 147456\tMean reward: 6.875\n",
      "Step: 155648\tMean reward: 7.84375\n",
      "Step: 163840\tMean reward: 10.28125\n",
      "Step: 172032\tMean reward: 11.28125\n",
      "Step: 180224\tMean reward: 11.0\n",
      "Step: 188416\tMean reward: 12.625\n",
      "Step: 196608\tMean reward: 12.46875\n",
      "Step: 204800\tMean reward: 12.21875\n",
      "Step: 212992\tMean reward: 13.1875\n",
      "Step: 221184\tMean reward: 13.0\n",
      "Step: 229376\tMean reward: 13.875\n",
      "Step: 237568\tMean reward: 13.6875\n",
      "Step: 245760\tMean reward: 14.0\n",
      "Step: 253952\tMean reward: 14.8125\n",
      "Step: 262144\tMean reward: 16.0625\n",
      "Step: 270336\tMean reward: 14.78125\n",
      "Step: 278528\tMean reward: 15.125\n",
      "Step: 286720\tMean reward: 14.25\n",
      "Step: 294912\tMean reward: 15.125\n",
      "Step: 303104\tMean reward: 14.0\n",
      "Step: 311296\tMean reward: 14.5\n",
      "Step: 319488\tMean reward: 14.53125\n",
      "Step: 327680\tMean reward: 14.71875\n",
      "Step: 335872\tMean reward: 14.5\n",
      "Step: 344064\tMean reward: 15.65625\n",
      "Step: 352256\tMean reward: 14.03125\n",
      "Step: 360448\tMean reward: 14.34375\n",
      "Step: 368640\tMean reward: 16.875\n",
      "Step: 376832\tMean reward: 14.625\n",
      "Step: 385024\tMean reward: 16.375\n",
      "Step: 393216\tMean reward: 16.25\n",
      "Step: 401408\tMean reward: 15.28125\n",
      "Step: 409600\tMean reward: 15.21875\n",
      "Step: 417792\tMean reward: 15.28125\n",
      "Step: 425984\tMean reward: 15.8125\n",
      "Step: 434176\tMean reward: 15.65625\n",
      "Step: 442368\tMean reward: 15.75\n",
      "Step: 450560\tMean reward: 15.125\n",
      "Step: 458752\tMean reward: 16.75\n",
      "Step: 466944\tMean reward: 15.96875\n",
      "Step: 475136\tMean reward: 17.40625\n",
      "Step: 483328\tMean reward: 16.125\n",
      "Step: 491520\tMean reward: 17.875\n",
      "Step: 499712\tMean reward: 16.15625\n",
      "Step: 507904\tMean reward: 17.25\n",
      "Step: 516096\tMean reward: 17.09375\n",
      "Step: 524288\tMean reward: 17.75\n",
      "Step: 532480\tMean reward: 17.0625\n",
      "Step: 540672\tMean reward: 17.15625\n",
      "Step: 548864\tMean reward: 16.46875\n",
      "Step: 557056\tMean reward: 17.28125\n",
      "Step: 565248\tMean reward: 18.125\n",
      "Step: 573440\tMean reward: 17.625\n",
      "Step: 581632\tMean reward: 17.75\n",
      "Step: 589824\tMean reward: 18.25\n",
      "Step: 598016\tMean reward: 18.09375\n",
      "Step: 606208\tMean reward: 18.28125\n",
      "Step: 614400\tMean reward: 16.34375\n",
      "Step: 622592\tMean reward: 17.90625\n",
      "Step: 630784\tMean reward: 18.5\n",
      "Step: 638976\tMean reward: 17.9375\n",
      "Step: 647168\tMean reward: 18.0625\n",
      "Step: 655360\tMean reward: 16.90625\n",
      "Step: 663552\tMean reward: 19.125\n",
      "Step: 671744\tMean reward: 18.90625\n",
      "Step: 679936\tMean reward: 19.3125\n",
      "Step: 688128\tMean reward: 19.375\n",
      "Step: 696320\tMean reward: 18.0\n",
      "Step: 704512\tMean reward: 17.59375\n",
      "Step: 712704\tMean reward: 16.75\n",
      "Step: 720896\tMean reward: 19.46875\n",
      "Step: 729088\tMean reward: 18.53125\n",
      "Step: 737280\tMean reward: 19.28125\n",
      "Step: 745472\tMean reward: 18.25\n",
      "Step: 753664\tMean reward: 18.375\n",
      "Step: 761856\tMean reward: 19.9375\n",
      "Step: 770048\tMean reward: 19.0\n",
      "Step: 778240\tMean reward: 19.625\n",
      "Step: 786432\tMean reward: 17.53125\n",
      "Step: 794624\tMean reward: 18.71875\n",
      "Step: 802816\tMean reward: 18.90625\n",
      "Step: 811008\tMean reward: 17.71875\n",
      "Step: 819200\tMean reward: 19.28125\n",
      "Step: 827392\tMean reward: 18.15625\n",
      "Step: 835584\tMean reward: 20.09375\n",
      "Step: 843776\tMean reward: 18.1875\n",
      "Step: 851968\tMean reward: 19.34375\n",
      "Step: 860160\tMean reward: 19.25\n",
      "Step: 868352\tMean reward: 19.375\n",
      "Step: 876544\tMean reward: 18.46875\n",
      "Step: 884736\tMean reward: 18.96875\n",
      "Step: 892928\tMean reward: 19.4375\n",
      "Step: 901120\tMean reward: 20.25\n",
      "Step: 909312\tMean reward: 20.8125\n",
      "Step: 917504\tMean reward: 18.9375\n",
      "Step: 925696\tMean reward: 19.9375\n",
      "Step: 933888\tMean reward: 19.0\n",
      "Step: 942080\tMean reward: 19.84375\n",
      "Step: 950272\tMean reward: 19.8125\n",
      "Step: 958464\tMean reward: 19.65625\n",
      "Step: 966656\tMean reward: 19.0\n",
      "Step: 974848\tMean reward: 19.3125\n",
      "Step: 983040\tMean reward: 20.25\n",
      "Step: 991232\tMean reward: 19.53125\n",
      "Step: 999424\tMean reward: 19.9375\n",
      "Step: 1007616\tMean reward: 22.1875\n",
      "Step: 1015808\tMean reward: 19.3125\n",
      "Step: 1024000\tMean reward: 20.59375\n",
      "Step: 1032192\tMean reward: 21.03125\n",
      "Step: 1040384\tMean reward: 21.1875\n",
      "Step: 1048576\tMean reward: 20.875\n",
      "Step: 1056768\tMean reward: 20.96875\n",
      "Step: 1064960\tMean reward: 20.75\n",
      "Step: 1073152\tMean reward: 21.5\n",
      "Step: 1081344\tMean reward: 22.375\n",
      "Step: 1089536\tMean reward: 20.65625\n",
      "Step: 1097728\tMean reward: 21.09375\n",
      "Step: 1105920\tMean reward: 19.53125\n",
      "Step: 1114112\tMean reward: 20.75\n",
      "Step: 1122304\tMean reward: 21.34375\n",
      "Step: 1130496\tMean reward: 22.15625\n",
      "Step: 1138688\tMean reward: 20.4375\n",
      "Step: 1146880\tMean reward: 21.3125\n",
      "Step: 1155072\tMean reward: 21.6875\n",
      "Step: 1163264\tMean reward: 21.15625\n",
      "Step: 1171456\tMean reward: 21.25\n",
      "Step: 1179648\tMean reward: 20.40625\n",
      "Step: 1187840\tMean reward: 22.4375\n",
      "Step: 1196032\tMean reward: 22.9375\n",
      "Step: 1204224\tMean reward: 21.65625\n",
      "Step: 1212416\tMean reward: 21.125\n",
      "Step: 1220608\tMean reward: 22.03125\n",
      "Step: 1228800\tMean reward: 20.78125\n",
      "Step: 1236992\tMean reward: 19.3125\n",
      "Step: 1245184\tMean reward: 24.0\n",
      "Step: 1253376\tMean reward: 21.5\n",
      "Step: 1261568\tMean reward: 22.0\n",
      "Step: 1269760\tMean reward: 19.9375\n",
      "Step: 1277952\tMean reward: 21.15625\n",
      "Step: 1286144\tMean reward: 22.84375\n",
      "Step: 1294336\tMean reward: 23.40625\n",
      "Step: 1302528\tMean reward: 21.34375\n",
      "Step: 1310720\tMean reward: 21.59375\n",
      "Step: 1318912\tMean reward: 21.25\n",
      "Step: 1327104\tMean reward: 20.90625\n",
      "Step: 1335296\tMean reward: 22.71875\n",
      "Step: 1343488\tMean reward: 22.53125\n",
      "Step: 1351680\tMean reward: 22.8125\n",
      "Step: 1359872\tMean reward: 22.46875\n",
      "Step: 1368064\tMean reward: 21.8125\n",
      "Step: 1376256\tMean reward: 21.125\n",
      "Step: 1384448\tMean reward: 22.0625\n",
      "Step: 1392640\tMean reward: 22.53125\n",
      "Step: 1400832\tMean reward: 20.5\n",
      "Step: 1409024\tMean reward: 21.28125\n",
      "Step: 1417216\tMean reward: 21.53125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9240/1443240035.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;31m# Take step in environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mnext_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;31m# Store data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\School\\MSc\\Semester 3\\02456 - Deep Learning\\Project\\RL_Project_02456\\utils.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    255\u001b[0m \t\t\"\"\"\n\u001b[0;32m    256\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\School\\MSc\\Semester 3\\02456 - Deep Learning\\Project\\RL_Project_02456\\utils.py\u001b[0m in \u001b[0;36mstep_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m                 \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\School\\MSc\\Semester 3\\02456 - Deep Learning\\Project\\RL_Project_02456\\utils.py\u001b[0m in \u001b[0;36mstep_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m                 \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    513\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\School\\MSc\\Semester 3\\02456 - Deep Learning\\Project\\RL_Project_02456\\utils.py\u001b[0m in \u001b[0;36mstep_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m                 \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    498\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\School\\MSc\\Semester 3\\02456 - Deep Learning\\Project\\RL_Project_02456\\utils.py\u001b[0m in \u001b[0;36mstep_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m                 \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m                         \u001b[0minfos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reward'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrews\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\School\\MSc\\Semester 3\\02456 - Deep Learning\\Project\\RL_Project_02456\\utils.py\u001b[0m in \u001b[0;36mstep_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m                 \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\procgen\\lib\\site-packages\\gym3\\interop.py\u001b[0m in \u001b[0;36mstep_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[0mrew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mac\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\procgen\\lib\\site-packages\\gym3\\libenv.py\u001b[0m in \u001b[0;36mget_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlibenv_observe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_env\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m         \u001b[0minfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_copy_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define environment\n",
    "# check the utils.py file for info on arguments\n",
    "env = make_env(num_envs, num_levels=num_levels, env_name = envname)\n",
    "print('Observation space:', env.observation_space)\n",
    "print('Action space:', env.action_space.n)\n",
    "\n",
    "\n",
    "encoder_in = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "\n",
    "# Define network\n",
    "encoder = Encoder(encoder_in,feature_dim)\n",
    "policy = Policy(encoder,feature_dim,num_actions)\n",
    "policy.cuda()\n",
    "\n",
    "# Define optimizer\n",
    "# these are reasonable values but probably not optimal\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=5e-4, eps=1e-5)\n",
    "\n",
    "# Define temporary storage\n",
    "# we use this to collect transitions during each iteration\n",
    "storage = Storage(\n",
    "    env.observation_space.shape,\n",
    "    num_steps,\n",
    "    num_envs\n",
    ")\n",
    "\n",
    "# Run training\n",
    "obs = env.reset()\n",
    "step = 0\n",
    "while step < total_steps:\n",
    "\n",
    "  # Use policy to collect data for num_steps steps\n",
    "  policy.eval()\n",
    "  for _ in range(num_steps):\n",
    "    \n",
    "\n",
    "    # Use policy\n",
    "    action, log_prob, value = policy.act(obs)\n",
    "    \n",
    "    # Take step in environment\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "    # Store data\n",
    "    storage.store(obs, action, reward, done, info, log_prob, value)\n",
    "    \n",
    "    # Update current observation\n",
    "    obs = next_obs\n",
    "\n",
    "  # Add the last observation to collected data\n",
    "  _, _, value = policy.act(obs)\n",
    "  storage.store_last(obs, value)\n",
    "\n",
    "  # Compute return and advantage\n",
    "  storage.compute_return_advantage()\n",
    "\n",
    "  # Optimize policy\n",
    "  policy.train()\n",
    "  for epoch in range(num_epochs):\n",
    "\n",
    "    # Iterate over batches of transitions\n",
    "    generator = storage.get_generator(batch_size)\n",
    "    for batch in generator:\n",
    "      b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch\n",
    "\n",
    "      # Get current policy outputs\n",
    "      new_dist, new_value = policy(b_obs)\n",
    "      new_log_prob = new_dist.log_prob(b_action)\n",
    "\n",
    "      # Clipped policy objective\n",
    "      ratio = torch.exp((new_log_prob - b_log_prob))\n",
    "      surr1 = ratio * b_advantage\n",
    "      surr2 = torch.clamp(ratio, 1.0 - eps, 1.0 + eps) * b_advantage\n",
    "      pi_loss = - torch.min(surr1, surr2).mean()\n",
    "\n",
    "      # Clipped value function objective\n",
    "      value_loss = (b_returns - new_value).pow(2).mean()\n",
    "\n",
    "      # Entropy loss\n",
    "      entropy_loss = new_dist.entropy()\n",
    "\n",
    "      # Backpropagate losses\n",
    "      loss = torch.mul(value_coef, value_loss) + pi_loss - torch.mul(entropy_coef, entropy_loss)\n",
    "      loss.mean().backward()\n",
    "\n",
    "      # Clip gradients\n",
    "      torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)\n",
    "\n",
    "      # Update policy\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "  # Update stats\n",
    "  step += num_envs * num_steps\n",
    "  print(f'Step: {step}\\tMean reward: {storage.get_reward()}')\n",
    "\n",
    "print('Completed training!')\n",
    "torch.save(policy.state_dict(), 'checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICV7XNqxMRHM",
    "outputId": "e21fc757-121b-429c-b4fa-30408326a059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed training!\n"
     ]
    }
   ],
   "source": [
    "print('Completed training!')\n",
    "torch.save(policy.state_dict(), 'checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "zQXZ_vbFyRxt",
    "outputId": "abf4487b-d320-4f6f-e21a-57870881870a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "pic = np.moveaxis(np.array(batch[0][0,:,:,:].cpu()), 0, 2)\n",
    "plt.imshow(pic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAZrWuVGLTu-"
   },
   "source": [
    "Below cell can be used for policy evaluation and saves an episode to mp4 for you to view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zecOCkd7Jzt",
    "outputId": "b03af8d0-0f03-45da-ae0d-c06ceeda96aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average return: tensor(26.9608)\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "\n",
    "# Make evaluation environment\n",
    "eval_env = make_env(num_envs, start_level=num_levels, num_levels=num_levels, env_name= envname)\n",
    "obs = eval_env.reset()\n",
    "\n",
    "frames = []\n",
    "total_reward = []\n",
    "\n",
    "# Evaluate policy\n",
    "policy.eval()\n",
    "for _ in range(1024):\n",
    "\n",
    "  # Use policy\n",
    "  action, log_prob, value = policy.act(obs)\n",
    "\n",
    "  # Take step in environment\n",
    "  obs, reward, done, info = eval_env.step(action)\n",
    "  total_reward.append(torch.Tensor(reward))\n",
    "\n",
    "  # Render environment and store\n",
    "  frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()\n",
    "  frames.append(frame)\n",
    "\n",
    "# Calculate average return\n",
    "total_reward = torch.stack(total_reward).sum(0).mean(0)\n",
    "print('Average return:', total_reward)\n",
    "\n",
    "# Save frames as video\n",
    "frames = torch.stack(frames)\n",
    "imageio.mimsave('vid.mp4', frames, fps=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0-jEZ0z25v8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "getting_started_ppo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
